# Default values for skafka.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


global:
  monitorDomain: "{{ .Release.Name }}.wai"
  imageRegistry: docker.io              # bitnami 계열 글로벌

kafka:
  enabled: true
  image:
    registry: docker.io

  clusterId: ""
  existingKraftSecret: ""
  kraftVersion: 1

  ##############################################################################################################
  # 상위설정과 하단의 controller, broker 개별 섹션에 동일한 옵션이 있을 때는, 하위설정이 우선적용됨 
  # 하위설정이 빈 값일 때만 상위설정 적용
  ##############################################################################################################

  # server.properties 전체 덮어쓰기. 빈 값{}은 default.
  config: {}
  
  # server.properties의 특정 키만 덮어쓰기. 사실상 extraConfig
  overrideConfiguration: # {}
    ########################################################
    # 모든 replications.factor 관련 설정 방법

    # 브로커 수: n
    # replications.factor 권장값: n-1
    # replications.factor 허용값: n과 같거나 작아야 함 && 최소 1
    
    # 이미 생성된 topic의 replication.factor는 여기서 변경불가. kafka-reassign-partitions.sh 스크립트를 사용하여 수동변경 가능.
    # 중요한 topic일 수록 replication.factor가 크면 좋음(e.g. __consumer_offset, connect-offset 등 연결정보 토픽)
    ########################################################
    # __consumer_offset의 복제 수 (default: 3)
    offsets.topic.replication.factor: 1
    # 신규생성 토픽의 파티션 수  (default: 1)
    num.partitions: 1
    # 신규생성 토픽의 복제 수    (default: 1)
    default.replication.factor: 1
    # 신규생성 토픽의 segment(roll) 분할 기간 단위  (default: 7일, 604800000)
    log.roll.ms: 172800000  # 2일
    # 신규생성 토픽에서 Record의 메타데이터 timestamp 타입 (default: CreateTime)
    log.message.timestamp.type: LogAppendTime


  existingConfigmap: ""
  secretConfig: ""
  existingSecretConfig: ""
  log4j2: ""
  existingLog4j2ConfigMap: ""
  
  # 빈값으로 두고 후속설정(controller.heapOpts)을 우선 사용한다.
  heapOpts: ""

  brokerRackAwareness:
    enabled: false
    cloudProvider: ""
    azureApiVersion: "2023-11-15"
  interBrokerProtocolVersion: ""

  # https://github.com/bitnami/charts/issues/19128
  listeners:
    client:
      protocol: PLAINTEXT  # SASL_PLAINTEXT
    controller:
      protocol: PLAINTEXT  # SASL_PLAINTEXT
    interbroker:
      protocol: PLAINTEXT  # SASL_PLAINTEXT
    external:
      protocol: PLAINTEXT  # SASL_PLAINTEXT
    extraListeners: []
    overrideListeners: ""
    advertisedListeners: ""
    securityProtocolMap: ""

  # sasl:
  # tls:
  extraEnvVars: []
  extraEnvVarsCM: ""
  extraEnvVarsSecret: ""
  extraVolumes: []
  extraVolumeMounts: []
  sidecars: []
  initContainers: []
  dnsPolicy: ""
  dnsConfig: {}

  defaultInitContainers:
    volumePermissions:
      enabled: false
    prepareConfig:
      containerSecurityContext:
        enabled: true
    autoDiscovery:
      enabled: true  # false시, 아래 네트워크 섹션 직접 설정 필요(nodeports, loadBalancerIps, externalIps 등)

  # controller: KRAFT모드에서 과거 zookeeper를 대체하는 노드를 의미.
  # Broker 기능을 포함할 수도 있고, 포함하지 않을 수도 있다. default는 combined node로 실행됨.
  controller:
    replicaCount: 3       # production ready에선 호스트 1대에 kafka 1개, pv 1개 씩만 배치할 것
    controllerOnly: false  # true: broker 기능을 포함하지 않는 controller-only노드로 실행 
                           # false(default): controller와 broker 기능을 모두 포함하는 combined node로 실행
    
    # KRaft 모드에서 컨트롤러/브로커가 서로를 찾을 때 사용하는 주소. default 빈문자열로 두면 자동.
    quorumBootstrapServers: ""
    # KRaft 모드에서 컨트롤러 쿼럼을 구성할 때 참여하는 노드의 최소 ID. default로두면 됨. 0부터 시작.
    minId: 0

    # controller(combined) 노드에서 단독설정 필요시 사용.
    # 여기말고 상위 공통 설정을 주로 사용할 것이므로 필요없음
    # config: {}
    # overrideConfiguration: {}
    # existingConfigmap: ""
    # secretConfig: ""
    # existingSecretConfig: ""

  
    # Production 권장값: 가용량의 20~40% 수준이면서, 1~6GB 범위를 넘지 않도록 한다. # 고사양 노드에서, percentage보다는 6GB 넘지 않는 것이 최우선사항.
    # Test실행최소사양: -Xmx512m -Xms512m
    # 가용량의 33% 지정 예시: -XX:InitialRAMPercentage=33 -XX:MaxRAMPercentage=33 
    # Percentage 옵션의 기준은 컨테이너 최대 가용량(requests.limits.memory, 없으면 노드 사양)이고, -Xms, -Xmx가 함께 지정되면 무시됨
    heapOpts: -Xmx512m -Xms512m

    # controller(combined) 노드에서 단독설정 필요시 사용.
    # 여기말고 상위 공통 설정을 주로 사용할 것이므로 필요없음
    # command: []
    # args: []
    # extraEnvVars: []
    # extraEnvVarsCM: ""
    # extraEnvVarsSecret: ""
    extraContainerPorts: []

    # livenessProbe: Production 배포시 일반적으로 꼭 필요한 편이나, 주의사항이 있음
    # - Container내부 앱이 무한루프, 데드락 등 "비정상상태이면서도 계속 실행중인 경우" Container를 종료&재실행하기 위함
    # - OOM kill처럼 "종료 signal이 있는 경우" 컨테이너가 알아서 종료&재실행되므로 livenessProbe가 필요한 경우는 아님
    # - (주의사항)대용량 트래픽 처리 시 처리지연으로 livenessProbe가 실패하여 불필요한 재시작이 반복될 수 있으니, 실패조건을 널널하게 주는 것이 좋음
    livenessProbe:
      enabled: true
    
    # readinessProbe: 필수는 아님
    # - Fail 지속시 해당 Container가 Service 엔드포인트에서 제외되어 외부 트래픽 유입이 차단됨 (Container는 종료되지 않음)
    # - 비정상 상태 앱의 통신을 사전차단하는 옵션
    # - 대부분의 상용앱에 retry 로직이 이미 있으므로 필수는 아님
    readinessProbe:
      enabled: true
    
    # 솔직히 필요없음. 꺼두는게 낫다.
    startupProbe:
      enabled: false

    lifecycleHooks: {}
   
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    resourcesPreset: "medium" # medium memory: 1024~1536Mi
    
    # Production시 최소 resources.requests 지정 필수!!
    # resources 미할당시, 테스트용 resourcePreset이 기본 적용됨. resources.requests만 설정해도 resourcePreset 무시됨.
    # resources.limits 할당시 힙메모리 고려할 것
    resources: # {}
      requests:
        cpu: 2
        memory: 512Mi
    ##   limits: {}

    # podSecurityContext:
    # containerSecurityContext:
    automountServiceAccountToken: true # autoDiscovery 사용시 true 필요
    # hostAliases: []
    # hostNetwork: false
    # hostIPC: false
    # podLabels: {}
    # podAnnotations: {}
    # topologyKey: ""
    # podAffinityPreset: ""
    # podAntiAffinityPreset: soft
    # nodeAffinityPreset:
    affinity: {}
    nodeSelector: {}
    tolerations: []
    # topologySpreadConstraints: []
    # terminationGracePeriodSeconds: ""
    # podManagementPolicy: Parallel
    # minReadySeconds: 0
    # priorityClassName: ""
    # runtimeClassName: ""
    # enableServiceLinks: true
    # schedulerName: ""
    updateStrategy:
      type: RollingUpdate
    extraVolumes: []
    extraVolumeMounts: []
    sidecars: []
    initContainers: []

    # autoscaling:

    # pdb: Pod의 자발적 중단(예: 노드 유지보수, 롤링 업데이트, 수동 삭제 등) 상황에서 동시에 중단될 수 있는 Pod의 수를 제한
    ## Kafka Pod Disruption Budget
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    ## @param controller.pdb.create Deploy a pdb object for the Kafka pod
    ## @param controller.pdb.minAvailable Minimum number/percentage of available Kafka replicas
    ## @param controller.pdb.maxUnavailable Maximum number/percentage of unavailable Kafka replicas
    ##
    pdb:
      create: true
      minAvailable: ""  # 고가용성 보장시 replication factor 최소 2 설정 후, "50%"이상 pod가 켜져있도록 설정
      maxUnavailable: ""

    persistentVolumeClaimRetentionPolicy:
      enabled: false
      whenScaled: Retain
      whenDeleted: Retain

    persistence:
      enabled: false               # production 배포시, true 필수
      size: 8Gi                    # 디스크 가용량의 70~80% 권장
      mountPath: /bitnami/kafka
    logPersistence:
      enabled: false
      size: 8Gi
      mountPath: /opt/bitnami/kafka/logs
  
  # controller 기능이 없는 순수 broker
  broker:
    replicaCount: 0
  
  # 공통 Service
  service:
    type: LoadBalancer
    ports:
      client: 9092
      controller: 9093
      interbroker: 9094
      external: 9095
    extraPorts: []
    nodePorts:
      client: ""
      external: ""
    sessionAffinity: None
    sessionAffinityConfig: {}
    clusterIP: ""
    loadBalancerIP: ""
    loadBalancerClass: ""
    loadBalancerSourceRanges: []
    allocateLoadBalancerNodePorts: true
    externalTrafficPolicy: Cluster
    annotations: {}
    headless:
      controller:
        annotations: {}
        labels: {}
      broker:
        annotations: {}
        labels: {}
      ipFamilies: []
      ipFamilyPolicy: ""


  ##################################################################################################
  # K8s기반 Kafka클러스터에서 외부 Client와 Broker 간 통신과정
    # 1. Client가 LoadBalancer의 주소(tcp-external)로 접근
    # 2. LoadBalancer는 Client의 최초 접근 트래픽을 Broker 1개에 랜덤매칭하여 전달                 (K8s의 LoadBalancer Service 기능)
    # 3. 해당 Broker는 모든 Broker의 advertised.listener(직접 접근 가능한 주소)를 Client에게 전달  (Kafka의 기능)
    #   - Tip) 모든 Broker는 서로의 advertised.listener 정보를 동기화하여 이미 갖고있음
    #   - Tip) 즉, Client는 어떤 경로로든 하나의 Broker에 접근하면 모든 Broker에 직접 접근 가능한 주소를 획득
    # 4. Kafka Client는 connection이 유지되는 동안 Broker로부터 받은 주소로 통신 (각 리더 파티션 배치에 따라 개별 Broker와 직접통신)

    # => kafka.service: 1번 과정의 "LoadBalancer 접근 방법"을 결정
    # => kakfa.externalAccess.controller.service: 4번 과정의 "개별 Broker 접근 방법"을 결정
  ##################################################################################################
  
  # externalAccess섹션은 [kafka server.properties의 advertised.listeners] 설정에 관여한다.
  
  # <kafka client가 "tcp-external"로 접근시 동작>
    # 첫번째 통신은 "tcp-external"포트를 사용
    # 이후 client는 advertised.listeners의 [EXTERNAL://IP:Port]로 통신
  
  # <네트워크 연결 검증시 주의 사항>
    # 1회성 통신인 [curl], [kcat토픽조회] 등은 advertised.listeners 주소를 사용하지 않으므로 externalAccess 설정에 오류가 있어도 정상 통신된다.
    # 2회 이상 통신하는 [kafka-topics.sh토픽조회], [produce], [consume] 등으로 테스트 필요
  externalAccess:
    enabled: true
    controller:
      forceExpose: false
      service:            # 노드 당 1개씩 생성되는 Service (advertised.listeners)
        type: NodePort
        ports:
          external: 9094  # "tcp-kafka"  (internally used)
        loadBalancerClass: ""
        loadBalancerIPs: []
        loadBalancerNames: []
        loadBalancerAnnotations: []
        loadBalancerSourceRanges: []
        allocateLoadBalancerNodePorts: true

        nodePorts: []    # broker 수 만큼 지정 (중복불가) # autoDiscovery 미사용시 필수 # e.g. [30003, 30004, 30005]  
        externalIPs: []  # broker 수 만큼 지정 # autoDiscovery 미사용시 설정 권장 # broker 별 실제 통신가능한 IP 직접 입력 (중복가능)

                         # externalAccess Nodeport모드 && autoDiscovery 미사용 && externalIPs 미등록시
                         # advertisedListener로 "EXTERNAL://{공인IP}:{개별nodeport}"가 자동등록된다.
                         # EC2에선 괜찮지만, 일반적으로 공인IP를 통한 포트포워딩이 없으므로 사용불가 
        useHostIPs: false
        usePodIPs: false
        domain: ""
        publishNotReadyAddresses: false
        labels: {}
        annotations: {}
        extraPorts: []
        ipFamilies: []
        ipFamilyPolicy: ""

    # Broker-Only의 externalAccess 설정 (섹션 동일)    
    broker:                          
      service:
        type: LoadBalancer

  networkPolicy:
    enabled: true
    allowExternal: true
    allowExternalEgress: true
    addExternalClientAccess: true
    extraIngress: []
    extraEgress: []
    #
    ingressPodMatchLabels: {}
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}

  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}
  rbac:
    create: true  # autoDiscovery 사용시 true 필요

  metrics:
    jmx:
      enabled: false
    serviceMonitor:
      enabled: false
    prometheusRule:
      enabled: false

  provisioning:
    enabled: false
    numPartitions: 1         # 초기 생성할 topic들의 partition 수 (broker의 default 설정 아님)
    replicationFactor: 1     # 초기 생성할 topic들의 replicationFactors (broker의 default 설정 아님)
    topics: []

connect:
  enabled: true
  kafka:
    create: false
  schema-registry:
    create: false
  replicaCount: 1
  image:
    repository: confluentinc/cp-kafka-connect
    pullPolicy: IfNotPresent
    tag: ""  # 7.2.2
  configMapPairs:  # 지정된 KEY 외에도 커스텀 환경변수를 넘길 수 있음
    CONNECT_BOOTSTRAP_SERVERS: "{{ .Release.Name }}-kafka:9092"
    CONNECT_REST_PORT: "8083"
    # KAFKA_HEAP_OPTS: "-Xms4G -Xmx4G"
    # 컨테이너 최대 가용량 대비 힙 비율 설정(requests.limits.memory, 없으면 노드 사양 기준)이고, -Xms, -Xmx가 함께 지정되면 무시됨
    # KAFKA_HEAP_OPTS: "-XX:InitialRAMPercentage=33 -XX:MaxRAMPercentage=33" 
  extraVolumeMounts: []
    # - name: plugin
    #   mountPath: /usr/share/confluent-hub-components
  extraVolumes: []
    # - name: plugin
    #   emptyDir: {}
  initContainers: []
    # - name: init-plugin
    #   image: confluentinc/cp-kafka-connect:7.2.2
    #   command:
    #     - sh
    #     - -c
    #     - |
    #       confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.7.4
    #       confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.5.4
    #       wget -P /usr/share/confluent-hub-components/confluentinc-kafka-connect-s3/ https://github.com/YunanJeong/kafka-connect-s3-without-topicname/releases/download/v10.5.0%2Bv1.0.0/topicless-timebasedpartitioner.jar
    #   volumeMounts:
    #     - name: plugin
    #       mountPath: /usr/share/confluent-hub-components

k8dashboard:
  enabled: true
  image:  # 글로벌 적용 불가
    registry: docker.io
  protocolHttp: true 
  service:
    externalPort: 8443
  serviceAccount:
    name: k8dash-admin
  extraArgs:
    - --token-ttl=86400
    - --enable-skip-login 
    - --enable-insecure-login
  tolerations:
  - key: type
    operator: "Equal"
    value: "ctrl"
    effect: "NoSchedule"

ui4kafka:
  enabled: true
  image:  # 글로벌 적용 불가
    registry: docker.io
  service:
    type: NodePort
    nodePort: 30080
  yamlApplicationConfig: false  # warning 메시지 발생하지만 괜찮음. # 아래서 Helm 내장객체인 {{ .Release.Name }} 사용하려면 false 설정필요
  myYamlApplicationConfig:
    kafka:
      clusters:
        - name: "{{ .Release.Name }}-kafka"
          bootstrapServers: "{{ .Release.Name }}-kafka:9092"
          # zookeeper: ReleaseName-zookeeper-headless:2181
          kafkaConnect:
            - name: connect
              address: http://{{ .Release.Name }}-connect:8083
          # metrics:
          #   port: 5556
          #   type: JMX
    auth:
      type: disabled
    management:
      health:
        ldap:
          enabled: false
  yamlApplicationConfigConfigMap:
    keyName: myconfig.yml
    name: skafka-ui-config  # configMapName
      
  tolerations:
    - key: type
      operator: "Equal"
      value: "ctrl"
      effect: "NoSchedule"

ingress:
  enabled: true
  annotations:
    spec.ingressClassName: traefik
    
    # kubernetes.io/ingress.class is deprecated
    
    # # minikube 설치용
    # kubernetes.io/ingress.class: nginx

    # # k3s 설치용
    # kubernetes.io/ingress.class: traefik

    # # k3d 설치용
    # ingress.kubernetes.io/ssl-redirect: "false"

    # # AWS EKS 설치용
    # kubernetes.io/ingress.class: alb
    # alb.ingress.kubernetes.io/group.name: public
    # alb.ingress.kubernetes.io/scheme: internet-facing
    # alb.ingress.kubernetes.io/target-type: ip
    # alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 9090}]'
